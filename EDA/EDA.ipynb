{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from wordcloud import WordCloud\n",
    "from textwrap import wrap\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "# \"..\\Data\\Banco-Preguntas-Icfes.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido del archivo (primeras 10 l√≠neas):\n",
      "\n",
      "L√≠nea 1: Pregunta: ¬øEn qu√© fechas se dar√° inicio al proceso de registro e inscripci√≥n a los ex√°menes?\n",
      "L√≠nea 2: Respuesta: Validar en la p√°gina www.icfes.gov.co / Evaluaciones y Elija el examen de su inter√©s.\n",
      "L√≠nea 3: \n",
      "L√≠nea 4: Pregunta:¬øC√≥mo puedo cambiar el correo de notificaciones asociado al usuario PRISMA de mi sede jornada?\n",
      "L√≠nea 5: Respuesta:A trav√©s del Chat, ingresando a la  p√°gina www.icfes.gov.co / Atenci√≥n y servicios a la ciudadan√≠a / canales de atenci√≥n / Chat.(Link:https://www.icfes.gov.co/atencion-y-servicios-a-la-ciudadania/canales-de-atencion/)\n",
      "L√≠nea 6: \n",
      "L√≠nea 7: Pregunta:¬øqu√© debo hacer para presentar las pruebas icfes por segunda vez?\n",
      "L√≠nea 8: Respuesta:Se puede registrar las veces que desee, despu√©s de la quinta inscripci√≥n la tarifa aumenta, validar en  la p√°gina www.icfes.gov.co / Evaluaciones y Elija el examen de su inter√©s.\n",
      "L√≠nea 9: \n",
      "L√≠nea 10: Pregunta:¬øEl examen saber 11¬∞, entrega certificado o constancia de asistencia?\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../Data/Banco-Preguntas-Icfes.txt\"\n",
    "\n",
    "# Leer el archivo l√≠nea por l√≠nea\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Mostrar las primeras 10 l√≠neas para verificar el formato\n",
    "print(\"Contenido del archivo (primeras 10 l√≠neas):\\n\")\n",
    "for i, line in enumerate(lines[:10]):\n",
    "    print(f\"L√≠nea {i+1}: {line.strip()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Primeras preguntas extra√≠das:\n",
      "                                            Pregunta\n",
      "0  ¬øEn qu√© fechas se dar√° inicio al proceso de re...\n",
      "1  ¬øC√≥mo puedo cambiar el correo de notificacione...\n",
      "2  ¬øqu√© debo hacer para presentar las pruebas icf...\n",
      "3  ¬øEl examen saber 11¬∞, entrega certificado o co...\n",
      "4                       ¬øDebo pagar las pruebas TyT?\n",
      "\n",
      "üìä Primeras respuestas extra√≠das:\n",
      "                                           Respuesta\n",
      "0  Validar en la p√°gina www.icfes.gov.co / Evalua...\n",
      "1  A trav√©s del Chat, ingresando a la  p√°gina www...\n",
      "2  Se puede registrar las veces que desee, despu√©...\n",
      "3  No, √∫nicamente se genera certificado de asiste...\n",
      "4  Valida las tarifas en la p√°gina www.icfes.gov....\n",
      "\n",
      "Total de preguntas extra√≠das: 26\n",
      "Total de respuestas extra√≠das: 26\n"
     ]
    }
   ],
   "source": [
    "# üìå 1Ô∏è‚É£ Inicializar listas para almacenar preguntas y respuestas\n",
    "preguntas = []\n",
    "respuestas = []\n",
    "pregunta_actual = \"\"\n",
    "respuesta_actual = \"\"\n",
    "leyendo_respuesta = False\n",
    "\n",
    "# üìå 2Ô∏è‚É£ Leer el archivo l√≠nea por l√≠nea y separar preguntas y respuestas\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Detectar preguntas\n",
    "        if line.startswith(\"Pregunta:\"):\n",
    "            if pregunta_actual:\n",
    "                preguntas.append(pregunta_actual.strip())\n",
    "                respuestas.append(respuesta_actual.strip())\n",
    "            pregunta_actual = line.replace(\"Pregunta:\", \"\").strip()\n",
    "            respuesta_actual = \"\"\n",
    "            leyendo_respuesta = False\n",
    "\n",
    "        # Detectar respuestas\n",
    "        elif line.startswith(\"Respuesta:\"):\n",
    "            leyendo_respuesta = True\n",
    "            respuesta_actual = line.replace(\"Respuesta:\", \"\").strip()\n",
    "\n",
    "        # Agregar contenido a la pregunta o respuesta\n",
    "        else:\n",
    "            if leyendo_respuesta:\n",
    "                respuesta_actual += \" \" + line.strip()\n",
    "            else:\n",
    "                pregunta_actual += \" \" + line.strip()\n",
    "\n",
    "# üìå 3Ô∏è‚É£ Agregar la √∫ltima pregunta y respuesta\n",
    "if pregunta_actual:\n",
    "    preguntas.append(pregunta_actual.strip())\n",
    "    respuestas.append(respuesta_actual.strip())\n",
    "\n",
    "# üìå 4Ô∏è‚É£ Crear DataFrames separados para preguntas y respuestas\n",
    "df_preguntas = pd.DataFrame({\"Pregunta\": preguntas})\n",
    "df_respuestas = pd.DataFrame({\"Respuesta\": respuestas})\n",
    "\n",
    "# üìå 5Ô∏è‚É£ Mostrar las primeras filas de cada DataFrame\n",
    "print(\"\\nüìä Primeras preguntas extra√≠das:\")\n",
    "print(df_preguntas.head())\n",
    "\n",
    "print(\"\\nüìä Primeras respuestas extra√≠das:\")\n",
    "print(df_respuestas.head())\n",
    "\n",
    "# üìå 6Ô∏è‚É£ Mostrar el n√∫mero total de preguntas y respuestas extra√≠das\n",
    "print(f\"\\nTotal de preguntas extra√≠das: {len(df_preguntas)}\")\n",
    "print(f\"Total de respuestas extra√≠das: {len(df_respuestas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto critico de limpieza, se borra los caracteres especiales, tildes, numeros y se pasa todo a minusculas\n",
    "Nota: para entregar los datos al RAG se puesden guardar un mapeo de los datos originales y el modelo trabaje con los datos limpios, y antes de entregar la respuesta al usuario busque en el mapeo el dato original y lo devuelva al usuario\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Primeras preguntas limpias:\n",
      "                                            Pregunta\n",
      "0  en que fechas se dara inicio al proceso de reg...\n",
      "1  como puedo cambiar el correo de notificaciones...\n",
      "2  que debo hacer para presentar las pruebas icfe...\n",
      "3  el examen saber entrega certificado constancia...\n",
      "4                         debo pagar las pruebas tyt\n",
      "\n",
      "üìä Primeras respuestas limpias:\n",
      "                                           Respuesta\n",
      "0  validar en la pagina wwwicfesgovco evaluacione...\n",
      "1  traves del chat ingresando la pagina wwwicfesg...\n",
      "2  se puede registrar las veces que desee despues...\n",
      "3  no unicamente se genera certificado de asisten...\n",
      "4  valida las tarifas en la pagina wwwicfesgovco ...\n"
     ]
    }
   ],
   "source": [
    "# üìå 1Ô∏è‚É£ Funci√≥n para limpiar el texto\n",
    "def limpiar_texto(texto):\n",
    "    if pd.isna(texto):  # Verificar si el texto es NaN o None\n",
    "        return \"\"\n",
    "    \n",
    "    texto = str(texto).lower()  # Convertir a min√∫sculas\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('utf-8')  # Eliminar tildes\n",
    "    texto = re.sub(r'\\d+', '', texto)  # Eliminar n√∫meros\n",
    "    texto = re.sub(r'[^\\w\\s]', '', texto)  # Eliminar caracteres especiales\n",
    "    texto = re.sub(r'\\b[a-zA-Z]\\b', '', texto)  # Eliminar palabras de 1 solo car√°cter\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()  # Eliminar espacios extra\n",
    "    \n",
    "    return texto\n",
    "\n",
    "# üìå 2Ô∏è‚É£ Aplicar la funci√≥n de limpieza a los DataFrames\n",
    "df_preguntas[\"Pregunta\"] = df_preguntas[\"Pregunta\"].apply(limpiar_texto)\n",
    "df_respuestas[\"Respuesta\"] = df_respuestas[\"Respuesta\"].apply(limpiar_texto)\n",
    "\n",
    "# üìå 3Ô∏è‚É£ Mostrar las primeras filas de los DataFrames limpios\n",
    "print(\"\\nPrimeras preguntas limpias:\")\n",
    "print(df_preguntas.head())\n",
    "\n",
    "print(\"\\nPrimeras respuestas limpias:\")\n",
    "print(df_respuestas.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\santi/nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\santi\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# üìå 4Ô∏è‚É£ Tokenizar preguntas y respuestas\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m tokens_preguntas = [token \u001b[38;5;28;01mfor\u001b[39;00m pregunta \u001b[38;5;129;01min\u001b[39;00m df_preguntas[\u001b[33m\"\u001b[39m\u001b[33mPregunta\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenizar_y_filtrar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpregunta\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m     12\u001b[39m tokens_respuestas = [token \u001b[38;5;28;01mfor\u001b[39;00m respuesta \u001b[38;5;129;01min\u001b[39;00m df_respuestas[\u001b[33m\"\u001b[39m\u001b[33mRespuesta\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenizar_y_filtrar(respuesta)]\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# üìå 5Ô∏è‚É£ Contar la frecuencia de palabras\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtokenizar_y_filtrar\u001b[39m\u001b[34m(texto)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenizar_y_filtrar\u001b[39m(texto):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexto\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokenizar el texto\u001b[39;00m\n\u001b[32m      7\u001b[39m     tokens = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word.isalpha() \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]  \u001b[38;5;66;03m# Filtrar stopwords y s√≠mbolos\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\santi/nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\santi\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# üìå 2Ô∏è‚É£ Definir stopwords en espa√±ol\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "# üìå 3Ô∏è‚É£ Funci√≥n para tokenizar y limpiar palabras\n",
    "def tokenizar_y_filtrar(texto):\n",
    "    tokens = word_tokenize(texto)  # Tokenizar el texto\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  # Filtrar stopwords y s√≠mbolos\n",
    "    return tokens\n",
    "\n",
    "# üìå 4Ô∏è‚É£ Tokenizar preguntas y respuestas\n",
    "tokens_preguntas = [token for pregunta in df_preguntas[\"Pregunta\"] for token in tokenizar_y_filtrar(pregunta)]\n",
    "tokens_respuestas = [token for respuesta in df_respuestas[\"Respuesta\"] for token in tokenizar_y_filtrar(respuesta)]\n",
    "\n",
    "# üìå 5Ô∏è‚É£ Contar la frecuencia de palabras\n",
    "frecuencia_preguntas = Counter(tokens_preguntas)\n",
    "frecuencia_respuestas = Counter(tokens_respuestas)\n",
    "\n",
    "# üìå 6Ô∏è‚É£ Obtener las 10 palabras m√°s comunes\n",
    "top_preguntas = frecuencia_preguntas.most_common(10)\n",
    "top_respuestas = frecuencia_respuestas.most_common(10)\n",
    "\n",
    "# üìå 7Ô∏è‚É£ Funci√≥n para graficar palabras m√°s comunes\n",
    "def graficar_palabras_comunes(palabras_comunes, titulo):\n",
    "    palabras, frecuencias = zip(*palabras_comunes)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=list(frecuencias), y=list(palabras), palette=\"viridis\")\n",
    "    plt.title(titulo)\n",
    "    plt.xlabel(\"Frecuencia\")\n",
    "    plt.ylabel(\"Palabras\")\n",
    "    plt.show()\n",
    "\n",
    "# üìå 8Ô∏è‚É£ Generar gr√°ficos para preguntas y respuestas\n",
    "graficar_palabras_comunes(top_preguntas, \"Palabras M√°s Comunes en Preguntas\")\n",
    "graficar_palabras_comunes(top_respuestas, \"Palabras M√°s Comunes en Respuestas\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icfes_pro-tNgj2KN3-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
