{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from wordcloud import WordCloud\n",
    "from textwrap import wrap\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "# \"..\\Data\\Banco-Preguntas-Icfes.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido del archivo (primeras 10 líneas):\n",
      "\n",
      "Línea 1: Pregunta: ¿En qué fechas se dará inicio al proceso de registro e inscripción a los exámenes?\n",
      "Línea 2: Respuesta: Validar en la página www.icfes.gov.co / Evaluaciones y Elija el examen de su interés.\n",
      "Línea 3: \n",
      "Línea 4: Pregunta:¿Cómo puedo cambiar el correo de notificaciones asociado al usuario PRISMA de mi sede jornada?\n",
      "Línea 5: Respuesta:A través del Chat, ingresando a la  página www.icfes.gov.co / Atención y servicios a la ciudadanía / canales de atención / Chat.(Link:https://www.icfes.gov.co/atencion-y-servicios-a-la-ciudadania/canales-de-atencion/)\n",
      "Línea 6: \n",
      "Línea 7: Pregunta:¿qué debo hacer para presentar las pruebas icfes por segunda vez?\n",
      "Línea 8: Respuesta:Se puede registrar las veces que desee, después de la quinta inscripción la tarifa aumenta, validar en  la página www.icfes.gov.co / Evaluaciones y Elija el examen de su interés.\n",
      "Línea 9: \n",
      "Línea 10: Pregunta:¿El examen saber 11°, entrega certificado o constancia de asistencia?\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../Data/Banco-Preguntas-Icfes.txt\"\n",
    "\n",
    "# Leer el archivo línea por línea\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Mostrar las primeras 10 líneas para verificar el formato\n",
    "print(\"Contenido del archivo (primeras 10 líneas):\\n\")\n",
    "for i, line in enumerate(lines[:10]):\n",
    "    print(f\"Línea {i+1}: {line.strip()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Primeras preguntas extraídas:\n",
      "                                            Pregunta\n",
      "0  ¿En qué fechas se dará inicio al proceso de re...\n",
      "1  ¿Cómo puedo cambiar el correo de notificacione...\n",
      "2  ¿qué debo hacer para presentar las pruebas icf...\n",
      "3  ¿El examen saber 11°, entrega certificado o co...\n",
      "4                       ¿Debo pagar las pruebas TyT?\n",
      "\n",
      "📊 Primeras respuestas extraídas:\n",
      "                                           Respuesta\n",
      "0  Validar en la página www.icfes.gov.co / Evalua...\n",
      "1  A través del Chat, ingresando a la  página www...\n",
      "2  Se puede registrar las veces que desee, despué...\n",
      "3  No, únicamente se genera certificado de asiste...\n",
      "4  Valida las tarifas en la página www.icfes.gov....\n",
      "\n",
      "Total de preguntas extraídas: 26\n",
      "Total de respuestas extraídas: 26\n"
     ]
    }
   ],
   "source": [
    "# 📌 1️⃣ Inicializar listas para almacenar preguntas y respuestas\n",
    "preguntas = []\n",
    "respuestas = []\n",
    "pregunta_actual = \"\"\n",
    "respuesta_actual = \"\"\n",
    "leyendo_respuesta = False\n",
    "\n",
    "# 📌 2️⃣ Leer el archivo línea por línea y separar preguntas y respuestas\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Detectar preguntas\n",
    "        if line.startswith(\"Pregunta:\"):\n",
    "            if pregunta_actual:\n",
    "                preguntas.append(pregunta_actual.strip())\n",
    "                respuestas.append(respuesta_actual.strip())\n",
    "            pregunta_actual = line.replace(\"Pregunta:\", \"\").strip()\n",
    "            respuesta_actual = \"\"\n",
    "            leyendo_respuesta = False\n",
    "\n",
    "        # Detectar respuestas\n",
    "        elif line.startswith(\"Respuesta:\"):\n",
    "            leyendo_respuesta = True\n",
    "            respuesta_actual = line.replace(\"Respuesta:\", \"\").strip()\n",
    "\n",
    "        # Agregar contenido a la pregunta o respuesta\n",
    "        else:\n",
    "            if leyendo_respuesta:\n",
    "                respuesta_actual += \" \" + line.strip()\n",
    "            else:\n",
    "                pregunta_actual += \" \" + line.strip()\n",
    "\n",
    "# 📌 3️⃣ Agregar la última pregunta y respuesta\n",
    "if pregunta_actual:\n",
    "    preguntas.append(pregunta_actual.strip())\n",
    "    respuestas.append(respuesta_actual.strip())\n",
    "\n",
    "# 📌 4️⃣ Crear DataFrames separados para preguntas y respuestas\n",
    "df_preguntas = pd.DataFrame({\"Pregunta\": preguntas})\n",
    "df_respuestas = pd.DataFrame({\"Respuesta\": respuestas})\n",
    "\n",
    "# 📌 5️⃣ Mostrar las primeras filas de cada DataFrame\n",
    "print(\"\\n📊 Primeras preguntas extraídas:\")\n",
    "print(df_preguntas.head())\n",
    "\n",
    "print(\"\\n📊 Primeras respuestas extraídas:\")\n",
    "print(df_respuestas.head())\n",
    "\n",
    "# 📌 6️⃣ Mostrar el número total de preguntas y respuestas extraídas\n",
    "print(f\"\\nTotal de preguntas extraídas: {len(df_preguntas)}\")\n",
    "print(f\"Total de respuestas extraídas: {len(df_respuestas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto critico de limpieza, se borra los caracteres especiales, tildes, numeros y se pasa todo a minusculas\n",
    "Nota: para entregar los datos al RAG se puesden guardar un mapeo de los datos originales y el modelo trabaje con los datos limpios, y antes de entregar la respuesta al usuario busque en el mapeo el dato original y lo devuelva al usuario\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Primeras preguntas limpias:\n",
      "                                            Pregunta\n",
      "0  en que fechas se dara inicio al proceso de reg...\n",
      "1  como puedo cambiar el correo de notificaciones...\n",
      "2  que debo hacer para presentar las pruebas icfe...\n",
      "3  el examen saber entrega certificado constancia...\n",
      "4                         debo pagar las pruebas tyt\n",
      "\n",
      "📊 Primeras respuestas limpias:\n",
      "                                           Respuesta\n",
      "0  validar en la pagina wwwicfesgovco evaluacione...\n",
      "1  traves del chat ingresando la pagina wwwicfesg...\n",
      "2  se puede registrar las veces que desee despues...\n",
      "3  no unicamente se genera certificado de asisten...\n",
      "4  valida las tarifas en la pagina wwwicfesgovco ...\n"
     ]
    }
   ],
   "source": [
    "# 📌 1️⃣ Función para limpiar el texto\n",
    "def limpiar_texto(texto):\n",
    "    if pd.isna(texto):  # Verificar si el texto es NaN o None\n",
    "        return \"\"\n",
    "    \n",
    "    texto = str(texto).lower()  # Convertir a minúsculas\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('utf-8')  # Eliminar tildes\n",
    "    texto = re.sub(r'\\d+', '', texto)  # Eliminar números\n",
    "    texto = re.sub(r'[^\\w\\s]', '', texto)  # Eliminar caracteres especiales\n",
    "    texto = re.sub(r'\\b[a-zA-Z]\\b', '', texto)  # Eliminar palabras de 1 solo carácter\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()  # Eliminar espacios extra\n",
    "    \n",
    "    return texto\n",
    "\n",
    "# 📌 2️⃣ Aplicar la función de limpieza a los DataFrames\n",
    "df_preguntas[\"Pregunta\"] = df_preguntas[\"Pregunta\"].apply(limpiar_texto)\n",
    "df_respuestas[\"Respuesta\"] = df_respuestas[\"Respuesta\"].apply(limpiar_texto)\n",
    "\n",
    "# 📌 3️⃣ Mostrar las primeras filas de los DataFrames limpios\n",
    "print(\"\\nPrimeras preguntas limpias:\")\n",
    "print(df_preguntas.head())\n",
    "\n",
    "print(\"\\nPrimeras respuestas limpias:\")\n",
    "print(df_respuestas.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\santi/nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\santi\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 📌 4️⃣ Tokenizar preguntas y respuestas\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m tokens_preguntas = [token \u001b[38;5;28;01mfor\u001b[39;00m pregunta \u001b[38;5;129;01min\u001b[39;00m df_preguntas[\u001b[33m\"\u001b[39m\u001b[33mPregunta\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenizar_y_filtrar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpregunta\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m     12\u001b[39m tokens_respuestas = [token \u001b[38;5;28;01mfor\u001b[39;00m respuesta \u001b[38;5;129;01min\u001b[39;00m df_respuestas[\u001b[33m\"\u001b[39m\u001b[33mRespuesta\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenizar_y_filtrar(respuesta)]\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 📌 5️⃣ Contar la frecuencia de palabras\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtokenizar_y_filtrar\u001b[39m\u001b[34m(texto)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenizar_y_filtrar\u001b[39m(texto):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexto\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokenizar el texto\u001b[39;00m\n\u001b[32m      7\u001b[39m     tokens = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word.isalpha() \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]  \u001b[38;5;66;03m# Filtrar stopwords y símbolos\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\santi/nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\santi\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# 📌 2️⃣ Definir stopwords en español\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "# 📌 3️⃣ Función para tokenizar y limpiar palabras\n",
    "def tokenizar_y_filtrar(texto):\n",
    "    tokens = word_tokenize(texto)  # Tokenizar el texto\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  # Filtrar stopwords y símbolos\n",
    "    return tokens\n",
    "\n",
    "# 📌 4️⃣ Tokenizar preguntas y respuestas\n",
    "tokens_preguntas = [token for pregunta in df_preguntas[\"Pregunta\"] for token in tokenizar_y_filtrar(pregunta)]\n",
    "tokens_respuestas = [token for respuesta in df_respuestas[\"Respuesta\"] for token in tokenizar_y_filtrar(respuesta)]\n",
    "\n",
    "# 📌 5️⃣ Contar la frecuencia de palabras\n",
    "frecuencia_preguntas = Counter(tokens_preguntas)\n",
    "frecuencia_respuestas = Counter(tokens_respuestas)\n",
    "\n",
    "# 📌 6️⃣ Obtener las 10 palabras más comunes\n",
    "top_preguntas = frecuencia_preguntas.most_common(10)\n",
    "top_respuestas = frecuencia_respuestas.most_common(10)\n",
    "\n",
    "# 📌 7️⃣ Función para graficar palabras más comunes\n",
    "def graficar_palabras_comunes(palabras_comunes, titulo):\n",
    "    palabras, frecuencias = zip(*palabras_comunes)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=list(frecuencias), y=list(palabras), palette=\"viridis\")\n",
    "    plt.title(titulo)\n",
    "    plt.xlabel(\"Frecuencia\")\n",
    "    plt.ylabel(\"Palabras\")\n",
    "    plt.show()\n",
    "\n",
    "# 📌 8️⃣ Generar gráficos para preguntas y respuestas\n",
    "graficar_palabras_comunes(top_preguntas, \"Palabras Más Comunes en Preguntas\")\n",
    "graficar_palabras_comunes(top_respuestas, \"Palabras Más Comunes en Respuestas\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icfes_pro-tNgj2KN3-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
