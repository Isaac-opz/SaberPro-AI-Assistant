{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from wordcloud import WordCloud\n",
    "from textwrap import wrap\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "# \"..\\Data\\Banco-Preguntas-Icfes.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido del archivo (primeras 10 líneas):\n",
      "\n",
      "Línea 1: Pregunta: ¿En qué fechas se dará inicio al proceso de registro e inscripción a los exámenes?\n",
      "Línea 2: Respuesta: Validar en la página www.icfes.gov.co / Evaluaciones y Elija el examen de su interés.\n",
      "Línea 3: \n",
      "Línea 4: Pregunta:¿Cómo puedo cambiar el correo de notificaciones asociado al usuario PRISMA de mi sede jornada?\n",
      "Línea 5: Respuesta:A través del Chat, ingresando a la  página www.icfes.gov.co / Atención y servicios a la ciudadanía / canales de atención / Chat.(Link:https://www.icfes.gov.co/atencion-y-servicios-a-la-ciudadania/canales-de-atencion/)\n",
      "Línea 6: \n",
      "Línea 7: Pregunta:¿qué debo hacer para presentar las pruebas icfes por segunda vez?\n",
      "Línea 8: Respuesta:Se puede registrar las veces que desee, después de la quinta inscripción la tarifa aumenta, validar en  la página www.icfes.gov.co / Evaluaciones y Elija el examen de su interés.\n",
      "Línea 9: \n",
      "Línea 10: Pregunta:¿El examen saber 11°, entrega certificado o constancia de asistencia?\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../Data/Banco-Preguntas-Icfes.txt\"\n",
    "\n",
    "# Leer el archivo línea por línea\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Mostrar las primeras 10 líneas para verificar el formato\n",
    "print(\"Contenido del archivo (primeras 10 líneas):\\n\")\n",
    "for i, line in enumerate(lines[:10]):\n",
    "    print(f\"Línea {i+1}: {line.strip()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Pregunta  \\\n",
      "0  ¿En qué fechas se dará inicio al proceso de re...   \n",
      "1  ¿Cómo puedo cambiar el correo de notificacione...   \n",
      "2  ¿qué debo hacer para presentar las pruebas icf...   \n",
      "3  ¿El examen saber 11°, entrega certificado o co...   \n",
      "4                       ¿Debo pagar las pruebas TyT?   \n",
      "\n",
      "                                           Respuesta  \n",
      "0  Validar en la página www.icfes.gov.co / Evalua...  \n",
      "1  A través del Chat, ingresando a la  página www...  \n",
      "2  Se puede registrar las veces que desee, despué...  \n",
      "3  No, únicamente se genera certificado de asiste...  \n",
      "4  Valida las tarifas en la página www.icfes.gov....  \n",
      "\n",
      "📊 Total de preguntas extraídas: 26\n",
      "📊 Total de respuestas extraídas: 26\n"
     ]
    }
   ],
   "source": [
    "# 📌 2️⃣ Inicializar listas para almacenar preguntas y respuestas\n",
    "preguntas = []\n",
    "respuestas = []\n",
    "pregunta_actual = \"\"\n",
    "respuesta_actual = \"\"\n",
    "leyendo_respuesta = False\n",
    "\n",
    "# 📌 3️⃣ Leer el archivo línea por línea y separar preguntas y respuestas\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Detectar preguntas\n",
    "        if line.startswith(\"Pregunta:\"):\n",
    "            if pregunta_actual:\n",
    "                preguntas.append(pregunta_actual.strip())\n",
    "                respuestas.append(respuesta_actual.strip())\n",
    "            pregunta_actual = line.replace(\"Pregunta:\", \"\").strip()\n",
    "            respuesta_actual = \"\"\n",
    "            leyendo_respuesta = False\n",
    "\n",
    "        # Detectar respuestas\n",
    "        elif line.startswith(\"Respuesta:\"):\n",
    "            leyendo_respuesta = True\n",
    "            respuesta_actual = line.replace(\"Respuesta:\", \"\").strip()\n",
    "\n",
    "        # Agregar contenido a la pregunta o respuesta\n",
    "        else:\n",
    "            if leyendo_respuesta:\n",
    "                respuesta_actual += \" \" + line.strip()\n",
    "            else:\n",
    "                pregunta_actual += \" \" + line.strip()\n",
    "\n",
    "# 📌 4️⃣ Agregar la última pregunta y respuesta\n",
    "if pregunta_actual:\n",
    "    preguntas.append(pregunta_actual.strip())\n",
    "    respuestas.append(respuesta_actual.strip())\n",
    "\n",
    "# 📌 5️⃣ Crear un DataFrame con preguntas y respuestas\n",
    "df = pd.DataFrame({\"Pregunta\": preguntas, \"Respuesta\": respuestas})\n",
    "\n",
    "# 📌 6️⃣ Mostrar las primeras filas para verificar los datos\n",
    "print(df.head())\n",
    "\n",
    "# 📌 7️⃣ Mostrar el número total de preguntas y respuestas extraídas\n",
    "print(f\"\\n📊 Total de preguntas extraídas: {len(preguntas)}\")\n",
    "print(f\"📊 Total de respuestas extraídas: {len(respuestas)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Pregunta_Limpia  \\\n",
      "0  en que fechas se dara inicio al proceso de reg...   \n",
      "1  como puedo cambiar el correo de notificaciones...   \n",
      "2  que debo hacer para presentar las pruebas icfe...   \n",
      "3  el examen saber 11 entrega certificado o const...   \n",
      "4                         debo pagar las pruebas tyt   \n",
      "\n",
      "                                    Respuesta_Limpia  \n",
      "0  validar en la pagina www icfes gov co evaluaci...  \n",
      "1  a traves del chat ingresando a la pagina www i...  \n",
      "2  se puede registrar las veces que desee despues...  \n",
      "3  no unicamente se genera certificado de asisten...  \n",
      "4  valida las tarifas en la pagina www icfes gov ...  \n"
     ]
    }
   ],
   "source": [
    "# 📌 1️⃣ Función para eliminar tildes y caracteres especiales\n",
    "def limpiar_texto(texto):\n",
    "    texto = str(texto).lower()  # Convertir a minúsculas\n",
    "    texto = re.sub(r'\\W+', ' ', texto)  # Eliminar caracteres especiales (excepto letras y espacios)\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('utf-8')  # Eliminar tildes\n",
    "    return texto.strip()  # Eliminar espacios extra\n",
    "\n",
    "# 📌 2️⃣ Aplicar la función a las columnas del DataFrame\n",
    "df[\"Pregunta_Limpia\"] = df[\"Pregunta\"].apply(limpiar_texto)\n",
    "df[\"Respuesta_Limpia\"] = df[\"Respuesta\"].apply(limpiar_texto)\n",
    "\n",
    "# 📌 3️⃣ Mostrar las primeras filas para verificar los datos limpios\n",
    "print(df[[\"Pregunta_Limpia\", \"Respuesta_Limpia\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\santi/nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\santi\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 📌 4️⃣ Tokenizar preguntas y respuestas limpias\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m tokens_preguntas = [token \u001b[38;5;28;01mfor\u001b[39;00m pregunta \u001b[38;5;129;01min\u001b[39;00m df[\u001b[33m\"\u001b[39m\u001b[33mPregunta_Limpia\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenizar_y_filtrar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpregunta\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m     12\u001b[39m tokens_respuestas = [token \u001b[38;5;28;01mfor\u001b[39;00m respuesta \u001b[38;5;129;01min\u001b[39;00m df[\u001b[33m\"\u001b[39m\u001b[33mRespuesta_Limpia\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenizar_y_filtrar(respuesta)]\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 📌 5️⃣ Contar frecuencia de palabras\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtokenizar_y_filtrar\u001b[39m\u001b[34m(texto)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenizar_y_filtrar\u001b[39m(texto):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexto\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Tokenizar el texto\u001b[39;00m\n\u001b[32m      7\u001b[39m     tokens = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]  \u001b[38;5;66;03m# Filtrar palabras vacías\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\santi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\icfes_pro-tNgj2KN3-py3.12\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\santi/nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\santi\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\icfes_pro-tNgj2KN3-py3.12\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\santi\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# 📌 2️⃣ Definir stopwords en español\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "# 📌 3️⃣ Función para tokenizar y eliminar stopwords\n",
    "def tokenizar_y_filtrar(texto):\n",
    "    tokens = word_tokenize(texto)  # Tokenizar el texto\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Filtrar palabras vacías\n",
    "    return tokens\n",
    "\n",
    "# 📌 4️⃣ Tokenizar preguntas y respuestas limpias\n",
    "tokens_preguntas = [token for pregunta in df[\"Pregunta_Limpia\"] for token in tokenizar_y_filtrar(pregunta)]\n",
    "tokens_respuestas = [token for respuesta in df[\"Respuesta_Limpia\"] for token in tokenizar_y_filtrar(respuesta)]\n",
    "\n",
    "# 📌 5️⃣ Contar frecuencia de palabras\n",
    "frecuencia_palabras_preguntas = Counter(tokens_preguntas)\n",
    "frecuencia_palabras_respuestas = Counter(tokens_respuestas)\n",
    "\n",
    "# 📌 6️⃣ Obtener las 10 palabras más comunes en cada conjunto\n",
    "palabras_comunes_preguntas = frecuencia_palabras_preguntas.most_common(10)\n",
    "palabras_comunes_respuestas = frecuencia_palabras_respuestas.most_common(10)\n",
    "\n",
    "# 📌 7️⃣ Mostrar los resultados\n",
    "print(\"\\n📊 Palabras más comunes en preguntas:\")\n",
    "for palabra, freq in palabras_comunes_preguntas:\n",
    "    print(f\"{palabra}: {freq}\")\n",
    "\n",
    "print(\"\\n📊 Palabras más comunes en respuestas:\")\n",
    "for palabra, freq in palabras_comunes_respuestas:\n",
    "    print(f\"{palabra}: {freq}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icfes_pro-tNgj2KN3-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
